可以使用 Scala 和 Spark 来实现抽取 shtd_store 库中 sku_info 表的增量数据，并且将这些数据导入到 Hive 的 ods 库中的 sku_info 表中。具体流程如下：

1. 首先需要连接 shtd_store 库和 Hive 的 ods 库，可以使用 SparkSQL 连接。连接需要使用到数据库的 JDBC 驱动，需要在 Spark 配置文件中配置，如下所示：

   
   spark.driver.extraClassPath /path/to/mysql-jdbc.jar
   

2. 然后使用 SparkSQL 读取 shtd_store.sku_info 表中的最新数据，并将读取到的数据注册为一个 Spark 的临时表，如下所示：

   
   val shtdStoreDf = sqlContext.read
     .jdbc("jdbc:mysql://shtd_store_host:3306/shtd_store", "sku_info",
           new Properties() {{
               setProperty("user", "shtd_user");
               setProperty("password", "shtd_password");
           }})
   
   shtdStoreDf.createOrReplaceTempView("shtd_store_sku_info")
   

3. 接着使用 SparkSQL 读取 Hive 中的 ods.sku_info 表中的最新数据，并将读取到的数据注册为一个 Spark 的临时表，如下所示：

   
   val hiveDf = sqlContext.read
     .jdbc("jdbc:hive2://hive_host:10000/ods", "sku_info",
           new Properties() {{
               setProperty("user", "hive_user");
               setProperty("password", "hive_password");
           }})
   
   hiveDf.createOrReplaceTempView("hive_sku_info")
   

4. 然后使用 SparkSQL 执行增量抽取操作，筛选出 shtd_store.sku_info 表中更新时间（update_time）大于 ods.sku_info 表中最大更新时间（max_update_time）的数据，并将这些数据写入到 Hive 中的 ods.sku_info 表中，如下所示：

   
   val maxUpdateTimeDf = sqlContext.sql("""
       SELECT MAX(update_time) AS max_update_time FROM hive_sku_info
   """)
   
   val maxUpdateTime = maxUpdateTimeDf.first().getAs[Timestamp]("max_update_time")
   
   val incrementalDf = sqlContext.sql(s"""
       SELECT * FROM shtd_store_sku_info WHERE update_time > '${maxUpdateTime.toString}'
   """)
   
   incrementalDf.write.mode("append")
     .jdbc("jdbc:hive2://hive_host:10000/ods", "sku_info",
           new Properties() {{
               setProperty("user", "hive_user");
               setProperty("password", "hive_password");
           }})
   

这样，就完成了从 shtd_store 仓库中抽取 sku_info 表的增量数据，并将这些数据导入到 Hive 的 ods 库中的 sku_info 表的操作。