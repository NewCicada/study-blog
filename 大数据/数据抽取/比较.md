根据ods.user_info表中operate_time或create_time作为增量字段(即MySQL中每条数据取这两个时间中较大的那个时间作为增量字段去和ods里的这两个字段中较大的时间进行比较)，
只将新增的数据抽入，字段名称、类型不变，同时添加静态分区，分区字段类型为String，且值为当前比赛日的前一天日期（分区字段格式为yyyyMMdd）。使用scala和spark

import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession

object UserInfoIncrementalLoad {

  def main(args: Array[String]) {

    // 初始化SparkSession
    val spark = SparkSession.builder()
      .appName("UserInfoIncrementalLoad")
      .master("local[*]")
      .enableHiveSupport()
      .getOrCreate()

    import spark.implicits._

    // 设置当前日期为比赛日的前一天
    val currentDate = java.time.LocalDate.now.minusDays(1)
    val partitionField = currentDate.format(java.time.format.DateTimeFormatter.ofPattern("yyyyMMdd"))

    // 从MySQL中读取增量数据
    val mysqlDF = spark.read.format("jdbc")
      .option("url", "jdbc:mysql://localhost:3306/dbname")
      .option("driver", "com.mysql.jdbc.Driver")
      .option("dbtable", "ods.user_info")
      .option("user", "username")
      .option("password", "password")
      .load()
      .withColumn("incremental_time", greatest($"operate_time", $"create_time")) // 取operate_time和create_time中的较大值作为增量字段
      .filter($"incremental_time" > lit("2021-09-01")) // 过滤出新增数据

    // 添加静态分区，并保存至Hive表
    mysqlDF.withColumn("partition_field", lit(partitionField))
      .write.mode("append")
      .partitionBy("partition_field")
      .format("hive")
      .saveAsTable("dbname.user_info")
  }

}


说明：

1. enableHiveSupport() 方法开启了Hive的支持，允许使用Hive的元数据。

2. 使用 java.time.LocalDate 类获取当前日期，并使用 java.time.format.DateTimeFormatter 格式化日期字符串。

3. greatest() 函数取两个字段中的较大值作为增量字段。

4. filter() 函数过滤出增量数据。

5. withColumn() 函数添加一个名为 partition_field 的静态分区，值为当前比赛日的前一天日期。

6. write() 函数将数据保存至Hive表，使用 mode("append") 选项表示追加写入数据，使用 partitionBy("partition_field") 选项表示以 partition_field 字段进行分区存储。